주제 (topic), 선행연구 (related work) 및 선행연구의 한계 (limitations)를 포함

1. 주제
동영상을 만화로 변환한다! 동영상을 넣으면 동영상 안에서 몇가지 프레임을 뽑아
컷툰으로 만들어주는것. 이때 뽑은 이미지 프레임을 사용자가 원하는 만화풍으로 
변환하여 실제 웹툰처럼 만들어줍니다.
저희는 이과정에서 gan이라는 딥러닝 모델을 사용합니다. 

2. 배경지식
->gan 이 뭐냐?
generative adversarial network = 생성 적대적 신경망. 
첫 단어인 generative : gan이 생성(generation) model이라는 것을 나타냄
두번째 단어 adversarial : gan이 두 개의 모델을 적대적(adversarial)으로 경쟁시키며 발전시킨다
마지막 network : 이 모델이 인공신경망(artificial neural network) 으로 만들어졌다

--> 위조 지폐범과 경찰을 생각해보자. 이 둘은 적대적인 경쟁관계. 
위조지폐범은 경찰을 속이기 위해서 점점 위조지폐 제조 기술을 발전시키고, 
경찰은 위조지폐범을 잡기 위해 점점 위조지폐를 찾는 기술을 발전시킨다.
시간이 흐르면 위조지폐범은 실제 지폐와 거의 똑같은 위조 지폐를 만들수 있을것.

이처럼 gan은 위조지폐범에 해당하는 생성자(Generator)와 
경찰에 해당하는 구분자 (Discriminator) 를 경쟁적으로 학습시킨다.
G의 목적은 진짜같은 가짜데이터를 만들어서 D를 속이는 것이며, 
D의 목적은 G가 만든 가짜데이터와 진짜데이터를 구분하는것.

이 둘을 함께 학습 시키면서 완벽한 가짜를 만들어내는 generator를 얻을 수 있다.
이것이 gan의 핵심적인 idea인 적대적 학습(adversarial learning)이다.

https://dreamgonfly.github.io/2018/03/17/gan-explained.html

3. 
gan은 image translation에도 적용할 수 있습니다. image translation은
흑백사진을 컬러 사진으로, 간단한 일러스트를 구체적인 사진으로 만들어내는 등
우리가 원하는대로 이미지를 바꾸어 재생성하는 것 입니다. 
 
이 그림은 아래 [그림4]는 2017년 UC버클리에서 GAN을 Image translation에 적용하기 위해 
GAN 구조에 원래 이미지의 형태를 잘 유지할 수 있는 조건을 추가한 cycleGAN 모델의 결과입니다.
어떤 사진을 주었을 때 모네 화풍의 그림으로 만들어내기도 하고, 
반대로 모네의 그림을 사진으로도 변환해내는 것을 볼 수 있죠. 
마찬가지로 얼룩말의 사진에서 얼룩무늬를 지워 말로 바꾸고, 
여름 분위기의 사진을 겨울 분위기로 변환하는 것도 가능합니다.

https://www.samsungsds.com/global/ko/support/insights/Generative-adversarial-network-AI-3.html

4. 
그런데 이 cycle gan은 저희가 원하는 image translation을 수행하기에는 
큰 한계점이 있었습니다. 
1. 1:1 매칭밖에 안됨 --> 도메인 간 관계 복잡해지면 도메인 간 관계를 제대로 파악 X
2. 인스턴스 모양 변경 잘 안됨
3. 배경 제외 인스턴스만 깨끗하게 변경안됨
4. 인스턴스 여러개일 경우 인스턴스끼리 통합되어 변환 --> 불확실 변환

이러한 문제를 해결하기 위해 저희는 cycle gan보다 더 업그레이드 된 논문들을 분석했습니다.


5.
저희가 분석한 논문은 총 4개인데요. 

동영상을 컷툰으로 만들 때 프레임을 뽑는 논문 하나, 
이미지를 원하는 만화풍을 가진 이미지로 변환할 때 더 자연스럽게 하기위한 논문 두개,
이미지 변환시 결과 이미지의 해상도를 높이기 위한 논문 하나.
이렇게 총 4가지 논문을 분석했습니다. 이 네가지 논문을 간단히 설명해드리겠습니다.


6. 
먼저 첫번째 논문입니다. =key - frame extraction
동영상을 컷툰으로 만들때 아무 프레임이나 뽑아서 넣으면 만들어진 컷툰도
아무 내용이 없는 이상한 만화가 되기 때문에 최대한 동영상을 대표?하는 프레임을
뽑기 위한 논문. 
이 논문에서 사용된 기술은 동영상에서 프레임을 다 뽑은다음 프레임의 유사도를 측정하여 
clustering을 하고 나누어진 cluter의 크기가 전체 프레임 수의 10%보다 크면 
해당 cluster에서 key-frame을 선택합니다. 
이를 통해 중복된 frame을 제거하고 의미있는 frame을 뽑아낼 수 있습니다.


7. 두번째 논문입니다 = insta gan
기존 cycle gan의 가장 큰 문제인 이미지 속의 인스턴스의 모양을 변경하거나
변환하고자 하는 인스턴스의 수가 여러개 일때 제대로 변환되지 않는 문제를 해결.
이미지 안의 인스턴스 변환시 자연스러운 변환을 위한 논문. 
이 논문에서는 이미지 변환시 변환하는 인스턴스에 초점을 맞추어 
대상 인스턴스만 변환하고 배경이나 인스턴스의 방향같은 
백그라운드 context는 원본 그대로 유지하기위해 context preserving loss를 정의.


8.
세번째 논문입니다.  = cartoon gan
일반 사진에 비해 만화는 부드러운 색감과 날카로운 윤곽선를 가지고 있습니다.
이미지를 변환했을 때 더 만화처럼 보이게 하기위한 논문.
이 논문에서는 날카로운 윤곽선은 만화이미지에서의 중요한 특성이지만 
이미지 전체에서는 윤곽선이 적은 비율을 차지하기 때문에 
윤곽선에 가중치를 올리는 edge-promoting loss정의

// 이 논문의 한계는 test data가 자연적인 이미지로만 이루어져 있다는 것
--> 사람이나 동물 같은 인스턴스에도 적용될지 의문?  


9.
네번째 논문입니다. = large scale gan
image translation을 하면 해상도가 떨어지는데, 결과 이미지의 해상도를 높이기 위한 논문.
이논문에서는 스케일링과 truncation trick(절단 트릭)을 이용하여 해상도를 높임
// 이 논문의 한계는 해상도를 높이려면 배치크기를 늘리고 매개변수의 수를 늘려
모델을 훈련시키기 때문에 Gpu 메모리, 즉 비용이 많이 든다.
또한 GAN은 모델이 커지면 불안정성이 높아져서 모델 자체가 붕괴될 위험이 있어서
이 기술은 적용유무를 논의중에 있다.


10. 이렇게 gan기반의 neural network를 이용한 image translation을 위해 선행논문 4가지를
분석해보았습니다. 분석한 내용을 간단히 하나의 그림으로 나타내보았습니다.(현수가 만든 그림넣자)
저희는 어플로 구현할 계획이라 어플에 동영상을 업로드하면 저희가 만든 모델에서 video data를
입력으로 받아서 key-frame extraction을 통해 동영상 내용을 최대한 잘 나타내는 프레임을 뽑고
그 이미지를 insta-gan과 cartoon gan을 이용하여 (필요시)인스턴스 모양을 바꾸고 이미지를 사용자가 원하는
만화풍으로 변환한뒤, 만약 해상도 향상 기술을 넣는다면 그것까지 해서 
최종적으로 모두 변환된 프레임 이미지들을 컷툰으로 바꾸어 출력하는 것입니다.

image translation과 관계된 논문은 이 외에도 여러가지가 있어서 프로젝트를 진행하면서
필요시 다른 논문의 기술을 적용시켜 지금의 발표와는 조금 다른 기술을 적용할 수도 있습니다.

11. QnA : 지금까지 발표를 들어주셔서 감사합니다. 질문 있으시면 해주세요!!! 